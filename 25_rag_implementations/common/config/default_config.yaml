# Default Configuration for 25 RAG Implementation Project

ollama:
  host: "localhost:11434"
  default_model: "llama2"
  timeout: 60
  temperature: 0.7
  max_tokens: 512

embeddings:
  model: "all-MiniLM-L6-v2"
  device: "cpu"  # or "cuda" if GPU available
  batch_size: 32

vector_store:
  type: "chromadb"  # chromadb, pinecone, weaviate, qdrant
  persist_directory: "./vector_db"
  collection_prefix: "rag_"

retrieval:
  top_k: 5
  chunk_size: 512
  chunk_overlap: 50
  similarity_threshold: 0.7

generation:
  max_context_length: 4000
  response_max_tokens: 512
  temperature: 0.7
  top_p: 0.9

monitoring:
  enable_logging: true
  log_file: "./logs/performance.jsonl"
  log_level: "INFO"

testing:
  test_data_path: "./data/test_documents"
  benchmark_queries_path: "./data/benchmark_queries.json"
  performance_threshold:
    max_response_time: 10.0  # seconds
    min_confidence: 0.3
